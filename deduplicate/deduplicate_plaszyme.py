import pandas as pd
from collections import defaultdict, Counter

# === ç”¨æˆ·å‚æ•° ===
INPUT_CSV = "/Users/shulei/PycharmProjects/Dataset/deduplicate/PlaszymeDB_v0.2.csv"
OUTPUT_CSV = INPUT_CSV.replace(".csv", ".1_deduplicated.csv")
LOG_TXT = INPUT_CSV.replace(".csv", ".1_deduplication_log.txt")

# === ä¸»é”®åˆ—å®šä¹‰ ===
key_cols = ["plastic", "label", "sequence"]

# === åŠ è½½æ•°æ®å¹¶æ ‡æ³¨åŸå§‹è¡Œå· ===
df = pd.read_csv(INPUT_CSV)
df["original_index"] = df.index.astype(str)

# === ç§»é™¤ sequence ä¸ºç©ºçš„è¡Œï¼ˆå…¶ä½™ä¸¤åˆ—å¯ä¸ºç©ºï¼‰===
df = df[~df["sequence"].isnull()].copy()

# === åˆå¹¶åˆ—ä¸ºé™¤ä¸»é”®åˆ—å¤–çš„å…¶ä»–åˆ— ===
merge_cols = [col for col in df.columns if col not in key_cols]

# === ä¿ç•™åŸå§‹åˆ—é¡ºåº ===
column_order = list(df.columns)

# === åˆå§‹åŒ–ç»Ÿè®¡å®¹å™¨ ===
group_to_indices = defaultdict(list)
group_to_content = {}
source_combinations = []

# === åˆ†ç»„åˆå¹¶å‡½æ•° ===
def merge_group(group):
    group_key = tuple(group.iloc[0][col] for col in key_cols)
    group_to_indices[group_key] = group["original_index"].tolist()

    merged = {}
    for col in merge_cols:
        values = group[col].dropna().astype(str).unique()
        clean_values = [v.strip() for v in values if v.strip() and v.lower() != "nan"]
        merged[col] = "/".join(sorted(set(clean_values)))
    for col in key_cols:
        merged[col] = group.iloc[0][col]

    # è®°å½•æ¥æºç»„åˆï¼ˆç”¨äºç»Ÿè®¡ï¼‰
    combined_sources = group["source_name"].dropna().astype(str).tolist()
    combo = "/".join(sorted(set(s.strip() for entry in combined_sources for s in entry.split("/") if s.strip())))
    if combo:
        source_combinations.append(combo)

    group_to_content[group_key] = merged
    return pd.Series(merged)

# === æ‰§è¡Œå»é‡ ===
df_dedup = df.groupby(key_cols, dropna=False, group_keys=False).apply(merge_group).reset_index(drop=True)
df_dedup = df_dedup[column_order]  # æ¢å¤åˆ—é¡ºåº

# === æŸ¥æ‰¾ä»æœ‰é‡å¤çš„ä¸»é”®è¡Œï¼ˆç†è®ºä¸Šä¸åº”æœ‰ï¼‰ ===
dedup_keys = df_dedup[key_cols].apply(lambda row: tuple(row), axis=1)
duplicate_counts = dedup_keys.value_counts()
still_duplicates = duplicate_counts[duplicate_counts > 1]

# === æ¥æºç»„åˆç»Ÿè®¡ ===
source_combo_counter = Counter(source_combinations)

# === å†™å…¥æ—¥å¿—æ–‡ä»¶ ===
with open(LOG_TXT, "w") as f:
    f.write("=== ğŸ“Š å»é‡ç»Ÿè®¡ä¿¡æ¯ ===\n")
    f.write(f"- å»é‡ä¸»é”®å­—æ®µ: {', '.join(key_cols)}\n")
    f.write(f"- åŸå§‹æ€»è¡Œæ•°ï¼ˆå‰”é™¤ sequence ä¸ºç©ºçš„è¡Œåï¼‰: {len(df)}\n")
    f.write(f"- å»é‡åæ€»è¡Œæ•°: {len(df_dedup)}\n")
    f.write(f"- å…±åˆå¹¶é‡å¤ç»„æ•°: {sum(1 for v in group_to_indices.values() if len(v) > 1)}\n")
    f.write(f"- å…±åˆå¹¶è®°å½•æ•°: {sum(len(v)-1 for v in group_to_indices.values() if len(v) > 1)}\n\n")

    f.write("=== ğŸ” é‡å¤æ¥æºç»„åˆç»Ÿè®¡ (source_name) ===\n")
    for combo, count in source_combo_counter.most_common():
        f.write(f"{combo} : {count} ç»„\n")
    f.write("\n")

    f.write("=== ğŸ” åˆå¹¶è¯¦æƒ… ===\n\n")
    for key, indices in group_to_indices.items():
        if len(indices) <= 1:
            continue
        f.write(f"[Merged Group] Key = {key}\n")
        f.write(f" - Merged from lines: {', '.join(indices)}\n")
        merged_row = group_to_content[key]
        for col in df.columns:
            f.write(f"   {col}: {merged_row[col]}\n")
        f.write("\n")

    if not still_duplicates.empty:
        f.write("\n=== âš ï¸ åˆå¹¶åä»å­˜åœ¨é‡å¤ä¸»é”®çš„æ¡ç›® ===\n")
        for key, count in still_duplicates.items():
            f.write(f"Duplicate key: {key} â†’ Appeared {count} times\n")

# === ä¿å­˜æœ€ç»ˆç»“æœ ===
df_dedup.to_csv(OUTPUT_CSV, index=False)
print(f"âœ… å»é‡å®Œæˆï¼Œå…±åˆå¹¶ {sum(len(v)-1 for v in group_to_indices.values() if len(v) > 1)} æ¡è®°å½•")
print(f"ğŸ“ æ–°æ–‡ä»¶å·²ä¿å­˜ä¸º: {OUTPUT_CSV}")
print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶å·²ä¿å­˜ä¸º: {LOG_TXT}")